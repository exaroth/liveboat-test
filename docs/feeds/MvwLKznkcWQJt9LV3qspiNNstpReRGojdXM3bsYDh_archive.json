{
  "id": "MvwLKznkcWQJt9LV3qspiNNstpReRGojdXM3bsYDh",
  "title": "Kubernetes Blog",
  "displayTitle": "Dev - Kubernetes Blog",
  "url": "https://kubernetes.io/feed.xml",
  "feedLink": "https://kubernetes.io/",
  "is_query": false,
  "items": [
    {
      "title": "Kubernetes 1.32: Moving Volume Group Snapshots to Beta",
      "url": "https://kubernetes.io/blog/2024/12/18/kubernetes-1-32-volume-group-snapshot-beta/",
      "date": 1734480000,
      "author": "",
      "unread": true,
      "content": "\n<p>Volume group snapshots were <a href=\"https://kubernetes.io/blog/2023/05/08/kubernetes-1-27-volume-group-snapshot-alpha/\">introduced</a>\nas an Alpha feature with the Kubernetes 1.27 release.\nThe recent release of Kubernetes v1.32 moved that support to <strong>beta</strong>.\nThe support for volume group snapshots relies on a set of\n<a href=\"https://kubernetes-csi.github.io/docs/group-snapshot-restore-feature.html#volume-group-snapshot-apis\">extension APIs for group snapshots</a>.\nThese APIs allow users to take crash consistent snapshots for a set of volumes.\nBehind the scenes, Kubernetes uses a label selector to group multiple PersistentVolumeClaims\nfor snapshotting.\nA key aim is to allow you restore that set of snapshots to new volumes and\nrecover your workload based on a crash consistent recovery point.</p>\n<p>This new feature is only supported for <a href=\"https://kubernetes-csi.github.io/docs/\">CSI</a> volume drivers.</p>\n<h2 id=\"an-overview-of-volume-group-snapshots\">An overview of volume group snapshots</h2>\n<p>Some storage systems provide the ability to create a crash consistent snapshot of\nmultiple volumes. A group snapshot represents <em>copies</em> made from multiple volumes, that\nare taken at the same point-in-time. A group snapshot can be used either to rehydrate\nnew volumes (pre-populated with the snapshot data) or to restore existing volumes to\na previous state (represented by the snapshots).</p>\n<h2 id=\"why-add-volume-group-snapshots-to-kubernetes\">Why add volume group snapshots to Kubernetes?</h2>\n<p>The Kubernetes volume plugin system already provides a powerful abstraction that\nautomates the provisioning, attaching, mounting, resizing, and snapshotting of block\nand file storage.</p>\n<p>Underpinning all these features is the Kubernetes goal of workload portability:\nKubernetes aims to create an abstraction layer between distributed applications and\nunderlying clusters so that applications can be agnostic to the specifics of the\ncluster they run on and application deployment requires no cluster specific knowledge.</p>\n<p>There was already a <a href=\"https://kubernetes.io/docs/concepts/storage/volume-snapshots/\">VolumeSnapshot</a> API\nthat provides the ability to take a snapshot of a persistent volume to protect against\ndata loss or data corruption. However, there are other snapshotting functionalities\nnot covered by the VolumeSnapshot API.</p>\n<p>Some storage systems support consistent group snapshots that allow a snapshot to be\ntaken from multiple volumes at the same point-in-time to achieve write order consistency.\nThis can be useful for applications that contain multiple volumes. For example,\nan application may have data stored in one volume and logs stored in another volume.\nIf snapshots for the data volume and the logs volume are taken at different times,\nthe application will not be consistent and will not function properly if it is restored\nfrom those snapshots when a disaster strikes.</p>\n<p>It is true that you can quiesce the application first, take an individual snapshot from\neach volume that is part of the application one after the other, and then unquiesce the\napplication after all the individual snapshots are taken. This way, you would get\napplication consistent snapshots.</p>\n<p>However, sometimes the application quiesce can be so time consuming that you want to do it less frequently,\nor it may not be possible to quiesce an application at all.\nFor example, a user may want to run weekly backups with application quiesce\nand nightly backups without application quiesce but with consistent group support which\nprovides crash consistency across all volumes in the group.</p>\n<h2 id=\"kubernetes-apis-for-volume-group-snapshots\">Kubernetes APIs for volume group snapshots</h2>\n<p>Kubernetes' support for <em>volume group snapshots</em> relies on three API kinds that\nare used\nfor managing snapshots:</p>\n<dl>\n<dt>VolumeGroupSnapshot</dt>\n<dd>Created by a Kubernetes user (or perhaps by your own automation) to request\ncreation of a volume group snapshot for multiple persistent volume claims.\nIt contains information about the volume group snapshot operation such as the\ntimestamp when the volume group snapshot was taken and whether it is ready to use.\nThe creation and deletion of this object represents a desire to create or delete a\ncluster resource (a group snapshot).</dd>\n<dt>VolumeGroupSnapshotContent</dt>\n<dd>Created by the snapshot controller for a dynamically created VolumeGroupSnapshot.\nIt contains information about the volume group snapshot including the volume group\nsnapshot ID.\nThis object represents a provisioned resource on the cluster (a group snapshot).\nThe VolumeGroupSnapshotContent object binds to the VolumeGroupSnapshot for which it\nwas created with a one-to-one mapping.</dd>\n<dt>VolumeGroupSnapshotClass</dt>\n<dd>Created by cluster administrators to describe how volume group snapshots should be\ncreated, including the driver information, the deletion policy, etc.</dd>\n</dl>\n<p>These three API kinds are defined as\n<a href=\"https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/\">CustomResourceDefinitions</a>\n(CRDs).\nThese CRDs must be installed in a Kubernetes cluster for a CSI Driver to support\nvolume group snapshots.</p>\n<h2 id=\"what-components-are-needed-to-support-volume-group-snapshots\">What components are needed to support volume group snapshots</h2>\n<p>Volume group snapshots are implemented in the\n<a href=\"https://github.com/kubernetes-csi/external-snapshotter\">external-snapshotter</a> repository.\nImplementing volume group snapshots meant adding or changing several components:</p>\n<ul>\n<li>Added new CustomResourceDefinitions for VolumeGroupSnapshot and two supporting APIs.</li>\n<li>Volume group snapshot controller logic is added to the common snapshot controller.</li>\n<li>Adding logic to make CSI calls into the snapshotter sidecar controller.</li>\n</ul>\n<p>The volume snapshot controller and CRDs are deployed once per\ncluster, while the sidecar is bundled with each CSI driver.</p>\n<p>Therefore, it makes sense to deploy the volume snapshot controller and CRDs as a cluster addon.</p>\n<p>The Kubernetes project recommends that Kubernetes distributors\nbundle and deploy the volume snapshot controller and CRDs as part\nof their Kubernetes cluster management process (independent of any CSI Driver).</p>\n<h2 id=\"what-s-new-in-beta\">What's new in Beta?</h2>\n<ul>\n<li>\n<p>The VolumeGroupSnapshot feature in CSI spec moved to GA in the <a href=\"https://github.com/container-storage-interface/spec/releases/tag/v1.11.0\">v1.11.0 release</a>.</p>\n</li>\n<li>\n<p>The snapshot validation webhook was deprecated in external-snapshotter v8.0.0 and it is now removed.\nMost of the validation webhook logic was added as validation rules into the CRDs.\nMinimum required Kubernetes version is 1.25 for these validation rules.\nOne thing in the validation webhook not moved to CRDs is the prevention of creating\nmultiple default volume snapshot classes and multiple default volume group snapshot classes\nfor the same CSI driver.\nWith the removal of the validation webhook, an error will still be raised when dynamically\nprovisioning a VolumeSnapshot or VolumeGroupSnapshot when multiple default volume snapshot\nclasses or multiple default volume group snapshot classes for the same CSI driver exist.</p>\n</li>\n<li>\n<p>The <code>enable-volumegroup-snapshot</code> flag in the snapshot-controller and the CSI snapshotter\nsidecar has been replaced by a feature gate.\nSince VolumeGroupSnapshot is a new API, the feature moves to Beta but the feature gate is\ndisabled by default.\nTo use this feature, enable the feature gate by adding the flag <code>--feature-gates=CSIVolumeGroupSnapshot=true</code>\nwhen starting the snapshot-controller and the CSI snapshotter sidecar.</p>\n</li>\n<li>\n<p>The logic to dynamically create the VolumeGroupSnapshot and its corresponding individual\nVolumeSnapshot and VolumeSnapshotContent objects are moved from the CSI snapshotter to the common\nsnapshot-controller.\nNew RBAC rules are added to the common snapshot-controller and some RBAC rules are removed from\nthe CSI snapshotter sidecar accordingly.</p>\n</li>\n</ul>\n<h2 id=\"how-do-i-use-kubernetes-volume-group-snapshots\">How do I use Kubernetes volume group snapshots</h2>\n<h3 id=\"creating-a-new-group-snapshot-with-kubernetes\">Creating a new group snapshot with Kubernetes</h3>\n<p>Once a VolumeGroupSnapshotClass object is defined and you have volumes you want to\nsnapshot together, you may request a new group snapshot by creating a VolumeGroupSnapshot\nobject.</p>\n<p>The source of the group snapshot specifies whether the underlying group snapshot\nshould be dynamically created or if a pre-existing VolumeGroupSnapshotContent\nshould be used.</p>\n<p>A pre-existing VolumeGroupSnapshotContent is created by a cluster administrator.\nIt contains the details of the real volume group snapshot on the storage system which\nis available for use by cluster users.</p>\n<p>One of the following members in the source of the group snapshot must be set.</p>\n<ul>\n<li><code>selector</code> - a label query over PersistentVolumeClaims that are to be grouped\ntogether for snapshotting. This selector will be used to match the label\nadded to a PVC.</li>\n<li><code>volumeGroupSnapshotContentName</code> - specifies the name of a pre-existing\nVolumeGroupSnapshotContent object representing an existing volume group snapshot.</li>\n</ul>\n<h4 id=\"dynamically-provision-a-group-snapshot\">Dynamically provision a group snapshot</h4>\n<p>In the following example, there are two PVCs.</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-console\" data-lang=\"console\"><span style=\"display:flex;\"><span><span style=\"color:#888\">NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS VOLUMEATTRIBUTESCLASS AGE\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">pvc-0 Bound pvc-6e1f7d34-a5c5-4548-b104-01e72c72b9f2 100Mi RWO csi-hostpath-sc &lt;unset&gt; 2m15s\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">pvc-1 Bound pvc-abc640b3-2cc1-4c56-ad0c-4f0f0e636efa 100Mi RWO csi-hostpath-sc &lt;unset&gt; 2m7s\n</span></span></span></code></pre></div><p>Label the PVCs.</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-console\" data-lang=\"console\"><span style=\"display:flex;\"><span><span style=\"color:#000080;font-weight:bold\">%</span> kubectl label pvc pvc-0 <span style=\"color:#b8860b\">group</span><span style=\"color:#666\">=</span>myGroup\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">persistentvolumeclaim/pvc-0 labeled\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\"></span><span style=\"\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"\"></span><span style=\"color:#000080;font-weight:bold\">%</span> kubectl label pvc pvc-1 <span style=\"color:#b8860b\">group</span><span style=\"color:#666\">=</span>myGroup\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">persistentvolumeclaim/pvc-1 labeled\n</span></span></span></code></pre></div><p>For dynamic provisioning, a selector must be set so that the snapshot controller can find PVCs\nwith the matching labels to be snapshotted together.</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex;\"><span><span style=\"color:#008000;font-weight:bold\">apiVersion</span>:<span style=\"color:#bbb\"> </span>groupsnapshot.storage.k8s.io/v1beta1<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">kind</span>:<span style=\"color:#bbb\"> </span>VolumeGroupSnapshot<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">metadata</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>snapshot-daily-20241217<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">namespace</span>:<span style=\"color:#bbb\"> </span>demo-namespace<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">spec</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">volumeGroupSnapshotClassName</span>:<span style=\"color:#bbb\"> </span>csi-groupSnapclass<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">source</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">selector</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">matchLabels</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">group</span>:<span style=\"color:#bbb\"> </span>myGroup<span style=\"color:#bbb\">\n</span></span></span></code></pre></div><p>In the VolumeGroupSnapshot spec, a user can specify the VolumeGroupSnapshotClass which\nhas the information about which CSI driver should be used for creating the group snapshot.\nA VolumGroupSnapshotClass is required for dynamic provisioning.</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex;\"><span><span style=\"color:#008000;font-weight:bold\">apiVersion</span>:<span style=\"color:#bbb\"> </span>groupsnapshot.storage.k8s.io/v1beta1<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">kind</span>:<span style=\"color:#bbb\"> </span>VolumeGroupSnapshotClass<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">metadata</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>csi-groupSnapclass<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">annotations</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">kubernetes.io/description</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#b44\">&#34;Example group snapshot class&#34;</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">driver</span>:<span style=\"color:#bbb\"> </span>example.csi.k8s.io<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">deletionPolicy</span>:<span style=\"color:#bbb\"> </span>Delete<span style=\"color:#bbb\">\n</span></span></span></code></pre></div><p>As a result of the volume group snapshot creation, a corresponding VolumeGroupSnapshotContent\nobject will be created with a volumeGroupSnapshotHandle pointing to a resource on the storage\nsystem.</p>\n<p>Two individual volume snapshots will be created as part of the volume group snapshot creation.</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-console\" data-lang=\"console\"><span style=\"display:flex;\"><span><span style=\"color:#888\">NAME READYTOUSE SOURCEPVC RESTORESIZE SNAPSHOTCONTENT AGE\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">snapshot-0962a745b2bf930bb385b7b50c9b08af471f1a16780726de19429dd9c94eaca0 true pvc-0 100Mi snapcontent-0962a745b2bf930bb385b7b50c9b08af471f1a16780726de19429dd9c94eaca0 16m\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">snapshot-da577d76bd2106c410616b346b2e72440f6ec7b12a75156263b989192b78caff true pvc-1 100Mi snapcontent-da577d76bd2106c410616b346b2e72440f6ec7b12a75156263b989192b78caff 16m\n</span></span></span></code></pre></div><h4 id=\"importing-an-existing-group-snapshot-with-kubernetes\">Importing an existing group snapshot with Kubernetes</h4>\n<p>To import a pre-existing volume group snapshot into Kubernetes, you must also import\nthe corresponding individual volume snapshots.</p>\n<p>Identify the individual volume snapshot handles, manually construct a\nVolumeSnapshotContent object first, then create a VolumeSnapshot object pointing to\nthe VolumeSnapshotContent object. Repeat this for every individual volume snapshot.</p>\n<p>Then manually create a VolumeGroupSnapshotContent object, specifying the\nvolumeGroupSnapshotHandle and individual volumeSnapshotHandles already existing\non the storage system.</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex;\"><span><span style=\"color:#008000;font-weight:bold\">apiVersion</span>:<span style=\"color:#bbb\"> </span>groupsnapshot.storage.k8s.io/v1beta1<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">kind</span>:<span style=\"color:#bbb\"> </span>VolumeGroupSnapshotContent<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">metadata</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>static-group-content<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">spec</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">deletionPolicy</span>:<span style=\"color:#bbb\"> </span>Delete<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">driver</span>:<span style=\"color:#bbb\"> </span>hostpath.csi.k8s.io<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">source</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">groupSnapshotHandles</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">volumeGroupSnapshotHandle</span>:<span style=\"color:#bbb\"> </span>e8779136-a93e-11ef-9549-66940726f2fd<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">volumeSnapshotHandles</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- e8779147-a93e-11ef-9549-66940726f2fd<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- e8783cd0-a93e-11ef-9549-66940726f2fd<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">volumeGroupSnapshotRef</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>static-group-snapshot<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">namespace</span>:<span style=\"color:#bbb\"> </span>demo-namespace<span style=\"color:#bbb\">\n</span></span></span></code></pre></div><p>After that create a VolumeGroupSnapshot object pointing to the VolumeGroupSnapshotContent\nobject.</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex;\"><span><span style=\"color:#008000;font-weight:bold\">apiVersion</span>:<span style=\"color:#bbb\"> </span>groupsnapshot.storage.k8s.io/v1beta1<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">kind</span>:<span style=\"color:#bbb\"> </span>VolumeGroupSnapshot<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">metadata</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>static-group-snapshot<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">namespace</span>:<span style=\"color:#bbb\"> </span>demo-namespace<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">spec</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">source</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">volumeGroupSnapshotContentName</span>:<span style=\"color:#bbb\"> </span>static-group-content<span style=\"color:#bbb\">\n</span></span></span></code></pre></div><h3 id=\"how-to-use-group-snapshot-for-restore-in-kubernetes\">How to use group snapshot for restore in Kubernetes</h3>\n<p>At restore time, the user can request a new PersistentVolumeClaim to be created from\na VolumeSnapshot object that is part of a VolumeGroupSnapshot. This will trigger\nprovisioning of a new volume that is pre-populated with data from the specified\nsnapshot. The user should repeat this until all volumes are created from all the\nsnapshots that are part of a group snapshot.</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex;\"><span><span style=\"color:#008000;font-weight:bold\">apiVersion</span>:<span style=\"color:#bbb\"> </span>v1<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">kind</span>:<span style=\"color:#bbb\"> </span>PersistentVolumeClaim<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">metadata</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>examplepvc-restored-2024-12-17<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">namespace</span>:<span style=\"color:#bbb\"> </span>demo-namespace<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">spec</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">storageClassName</span>:<span style=\"color:#bbb\"> </span>example-foo-nearline<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">dataSource</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">name</span>:<span style=\"color:#bbb\"> </span>snapshot-0962a745b2bf930bb385b7b50c9b08af471f1a16780726de19429dd9c94eaca0<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">kind</span>:<span style=\"color:#bbb\"> </span>VolumeSnapshot<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">apiGroup</span>:<span style=\"color:#bbb\"> </span>snapshot.storage.k8s.io<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">accessModes</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>- ReadWriteOncePod<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">resources</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">requests</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">storage</span>:<span style=\"color:#bbb\"> </span>100Mi<span style=\"color:#bbb\"> </span><span style=\"color:#080;font-style:italic\"># must be enough storage to fit the existing snapshot</span><span style=\"color:#bbb\">\n</span></span></span></code></pre></div><h2 id=\"as-a-storage-vendor-how-do-i-add-support-for-group-snapshots-to-my-csi-driver\">As a storage vendor, how do I add support for group snapshots to my CSI driver?</h2>\n<p>To implement the volume group snapshot feature, a CSI driver <strong>must</strong>:</p>\n<ul>\n<li>Implement a new group controller service.</li>\n<li>Implement group controller RPCs: <code>CreateVolumeGroupSnapshot</code>, <code>DeleteVolumeGroupSnapshot</code>, and <code>GetVolumeGroupSnapshot</code>.</li>\n<li>Add group controller capability <code>CREATE_DELETE_GET_VOLUME_GROUP_SNAPSHOT</code>.</li>\n</ul>\n<p>See the <a href=\"https://github.com/container-storage-interface/spec/blob/master/spec.md\">CSI spec</a>\nand the <a href=\"https://kubernetes-csi.github.io/docs/\">Kubernetes-CSI Driver Developer Guide</a>\nfor more details.</p>\n<p>As mentioned earlier, it is strongly recommended that Kubernetes distributors\nbundle and deploy the volume snapshot controller and CRDs as part\nof their Kubernetes cluster management process (independent of any CSI Driver).</p>\n<p>As part of this recommended deployment process, the Kubernetes team provides a number of\nsidecar (helper) containers, including the\n<a href=\"https://kubernetes-csi.github.io/docs/external-snapshotter.html\">external-snapshotter sidecar container</a>\nwhich has been updated to support volume group snapshot.</p>\n<p>The external-snapshotter watches the Kubernetes API server for\nVolumeGroupSnapshotContent objects, and triggers <code>CreateVolumeGroupSnapshot</code> and\n<code>DeleteVolumeGroupSnapshot</code> operations against a CSI endpoint.</p>\n<h2 id=\"what-are-the-limitations\">What are the limitations?</h2>\n<p>The beta implementation of volume group snapshots for Kubernetes has the following limitations:</p>\n<ul>\n<li>Does not support reverting an existing PVC to an earlier state represented by\na snapshot (only supports provisioning a new volume from a snapshot).</li>\n<li>No application consistency guarantees beyond any guarantees provided by the storage system\n(e.g. crash consistency). See this <a href=\"https://github.com/kubernetes/community/blob/30d06f49fba22273f31b3c616b74cf8745c19b3d/wg-data-protection/data-protection-workflows-white-paper.md#quiesce-and-unquiesce-hooks\">doc</a>\nfor more discussions on application consistency.</li>\n</ul>\n<h2 id=\"what-s-next\">What’s next?</h2>\n<p>Depending on feedback and adoption, the Kubernetes project plans to push the volume\ngroup snapshot implementation to general availability (GA) in a future release.</p>\n<h2 id=\"how-can-i-learn-more\">How can I learn more?</h2>\n<ul>\n<li>The <a href=\"https://github.com/kubernetes/enhancements/tree/master/keps/sig-storage/3476-volume-group-snapshot\">design spec</a>\nfor the volume group snapshot feature.</li>\n<li>The <a href=\"https://github.com/kubernetes-csi/external-snapshotter\">code repository</a> for volume group\nsnapshot APIs and controller.</li>\n<li>CSI <a href=\"https://kubernetes-csi.github.io/docs/\">documentation</a> on the group snapshot feature.</li>\n</ul>\n<h2 id=\"how-do-i-get-involved\">How do I get involved?</h2>\n<p>This project, like all of Kubernetes, is the result of hard work by many contributors\nfrom diverse backgrounds working together. On behalf of SIG Storage, I would like to\noffer a huge thank you to the contributors who stepped up these last few quarters\nto help the project reach beta:</p>\n<ul>\n<li>Ben Swartzlander (<a href=\"https://github.com/bswartz\">bswartz</a>)</li>\n<li>Cici Huang (<a href=\"https://github.com/cici37\">cici37</a>)</li>\n<li>Hemant Kumar (<a href=\"https://github.com/gnufied\">gnufied</a>)</li>\n<li>James Defelice (<a href=\"https://github.com/jdef\">jdef</a>)</li>\n<li>Jan Šafránek (<a href=\"https://github.com/jsafrane\">jsafrane</a>)</li>\n<li>Madhu Rajanna (<a href=\"https://github.com/Madhu-1\">Madhu-1</a>)</li>\n<li>Manish M Yathnalli (<a href=\"https://github.com/manishym\">manishym</a>)</li>\n<li>Michelle Au (<a href=\"https://github.com/msau42\">msau42</a>)</li>\n<li>Niels de Vos (<a href=\"https://github.com/nixpanic\">nixpanic</a>)</li>\n<li>Leonardo Cecchi (<a href=\"https://github.com/leonardoce\">leonardoce</a>)</li>\n<li>Rakshith R (<a href=\"https://github.com/Rakshith-R\">Rakshith-R</a>)</li>\n<li>Raunak Shah (<a href=\"https://github.com/RaunakShah\">RaunakShah</a>)</li>\n<li>Saad Ali (<a href=\"https://github.com/saad-ali\">saad-ali</a>)</li>\n<li>Xing Yang (<a href=\"https://github.com/xing-yang\">xing-yang</a>)</li>\n<li>Yati Padia (<a href=\"https://github.com/yati1998\">yati1998</a>)</li>\n</ul>\n<p>For those interested in getting involved with the design and development of CSI or\nany part of the Kubernetes Storage system, join the\n<a href=\"https://github.com/kubernetes/community/tree/master/sig-storage\">Kubernetes Storage Special Interest Group</a> (SIG).\nWe always welcome new contributors.</p>\n<p>We also hold regular <a href=\"https://github.com/kubernetes/community/tree/master/wg-data-protection\">Data Protection Working Group meetings</a>.\nNew attendees are welcome to join our discussions.</p>",
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": ""
    },
    {
      "title": "Enhancing Kubernetes API Server Efficiency with API Streaming",
      "url": "https://kubernetes.io/blog/2024/12/17/kube-apiserver-api-streaming/",
      "date": 1734393600,
      "author": "",
      "unread": true,
      "content": "\n<p>Managing Kubernetes clusters efficiently is critical, especially as their size is growing.\nA significant challenge with large clusters is the memory overhead caused by <strong>list</strong> requests.</p>\n<p>In the existing implementation, the kube-apiserver processes <strong>list</strong> requests by assembling the entire response in-memory before transmitting any data to the client.\nBut what if the response body is substantial, say hundreds of megabytes? Additionally, imagine a scenario where multiple <strong>list</strong> requests flood in simultaneously, perhaps after a brief network outage.\nWhile <a href=\"https://kubernetes.io/docs/concepts/cluster-administration/flow-control/\">API Priority and Fairness</a> has proven to reasonably protect kube-apiserver from CPU overload, its impact is visibly smaller for memory protection.\nThis can be explained by the differing nature of resource consumption by a single API request - the CPU usage at any given time is capped by a constant, whereas memory, being uncompressible, can grow proportionally with the number of processed objects and is unbounded.\nThis situation poses a genuine risk, potentially overwhelming and crashing any kube-apiserver within seconds due to out-of-memory (OOM) conditions. To better visualize the issue, let's consider the below graph.</p>\n<figure class=\"diagram-large clickable-zoom\">\n<img src=\"https://kubernetes.io/blog/2024/12/17/kube-apiserver-api-streaming/kube-apiserver-memory_usage.png\"\nalt=\"Monitoring graph showing kube-apiserver memory usage\"/>\n</figure>\n<p>The graph shows the memory usage of a kube-apiserver during a synthetic test.\n(see the <a href=\"#the-synthetic-test\">synthetic test</a> section for more details).\nThe results clearly show that increasing the number of informers significantly boosts the server's memory consumption.\nNotably, at approximately 16:40, the server crashed when serving only 16 informers.</p>\n<h2 id=\"why-does-kube-apiserver-allocate-so-much-memory-for-list-requests\">Why does kube-apiserver allocate so much memory for list requests?</h2>\n<p>Our investigation revealed that this substantial memory allocation occurs because the server before sending the first byte to the client must:</p>\n<ul>\n<li>fetch data from the database,</li>\n<li>deserialize the data from its stored format,</li>\n<li>and finally construct the final response by converting and serializing the data into a client requested format</li>\n</ul>\n<p>This sequence results in significant temporary memory consumption.\nThe actual usage depends on many factors like the page size, applied filters (e.g. label selectors), query parameters, and sizes of individual objects.</p>\n<p>Unfortunately, neither <a href=\"https://kubernetes.io/docs/concepts/cluster-administration/flow-control/\">API Priority and Fairness</a> nor Golang's garbage collection or Golang memory limits can prevent the system from exhausting memory under these conditions.\nThe memory is allocated suddenly and rapidly, and just a few requests can quickly deplete the available memory, leading to resource exhaustion.</p>\n<p>Depending on how the API server is run on the node, it might either be killed through OOM by the kernel when exceeding the configured memory limits during these uncontrolled spikes, or if limits are not configured it might have even worse impact on the control plane node.\nAnd worst, after the first API server failure, the same requests will likely hit another control plane node in an HA setup with probably the same impact.\nPotentially a situation that is hard to diagnose and hard to recover from.</p>\n<h2 id=\"streaming-list-requests\">Streaming list requests</h2>\n<p>Today, we're excited to announce a major improvement.\nWith the graduation of the <em>watch list</em> feature to beta in Kubernetes 1.32, client-go users can opt-in (after explicitly enabling <code>WatchListClient</code> feature gate)\nto streaming lists by switching from <strong>list</strong> to (a special kind of) <strong>watch</strong> requests.</p>\n<p><strong>Watch</strong> requests are served from the <em>watch cache</em>, an in-memory cache designed to improve scalability of read operations.\nBy streaming each item individually instead of returning the entire collection, the new method maintains constant memory overhead.\nThe API server is bound by the maximum allowed size of an object in etcd plus a few additional allocations.\nThis approach drastically reduces the temporary memory usage compared to traditional <strong>list</strong> requests, ensuring a more efficient and stable system,\nespecially in clusters with a large number of objects of a given type or large average object sizes where despite paging memory consumption used to be high.</p>\n<p>Building on the insight gained from the synthetic test (see the <a href=\"#the-synthetic-test\">synthetic test</a>, we developed an automated performance test to systematically evaluate the impact of the <em>watch list</em> feature.\nThis test replicates the same scenario, generating a large number of Secrets with a large payload, and scaling the number of informers to simulate heavy <strong>list</strong> request patterns.\nThe automated test is executed periodically to monitor memory usage of the server with the feature enabled and disabled.</p>\n<p>The results showed significant improvements with the <em>watch list</em> feature enabled.\nWith the feature turned on, the kube-apiserver’s memory consumption stabilized at approximately <strong>2 GB</strong>.\nBy contrast, with the feature disabled, memory usage increased to approximately <strong>20GB</strong>, a <strong>10x</strong> increase!\nThese results confirm the effectiveness of the new streaming API, which reduces the temporary memory footprint.</p>\n<h2 id=\"enabling-api-streaming-for-your-component\">Enabling API Streaming for your component</h2>\n<p>Upgrade to Kubernetes 1.32. Make sure your cluster uses etcd in version 3.4.31+ or 3.5.13+.\nChange your client software to use watch lists. If your client code is written in Golang, you'll want to enable <code>WatchListClient</code> for client-go.\nFor details on enabling that feature, read <a href=\"https://kubernetes.io/blog/2024/08/12/feature-gates-in-client-go\">Introducing Feature Gates to Client-Go: Enhancing Flexibility and Control</a>.</p>\n<h2 id=\"what-s-next\">What's next?</h2>\n<p>In Kubernetes 1.32, the feature is enabled in kube-controller-manager by default despite its beta state.\nThis will eventually be expanded to other core components like kube-scheduler or kubelet; once the feature becomes generally available, if not earlier.\nOther 3rd-party components are encouraged to opt-in to the feature during the beta phase, especially when they are at risk of accessing a large number of resources or kinds with potentially large object sizes.</p>\n<p>For the time being, <a href=\"https://kubernetes.io/docs/concepts/cluster-administration/flow-control/\">API Priority and Fairness</a> assigns a reasonable small cost to <strong>list</strong> requests.\nThis is necessary to allow enough parallelism for the average case where <strong>list</strong> requests are cheap enough.\nBut it does not match the spiky exceptional situation of many and large objects.\nOnce the majority of the Kubernetes ecosystem has switched to <em>watch list</em>, the <strong>list</strong> cost estimation can be changed to larger values without risking degraded performance in the average case,\nand with that increasing the protection against this kind of requests that can still hit the API server in the future.</p>\n<h2 id=\"the-synthetic-test\">The synthetic test</h2>\n<p>In order to reproduce the issue, we conducted a manual test to understand the impact of <strong>list</strong> requests on kube-apiserver memory usage.\nIn the test, we created 400 Secrets, each containing 1 MB of data, and used informers to retrieve all Secrets.</p>\n<p>The results were alarming, only 16 informers were needed to cause the test server to run out of memory and crash, demonstrating how quickly memory consumption can grow under such conditions.</p>\n<p>Special shout out to <a href=\"https://github.com/deads2k\">@deads2k</a> for his help in shaping this feature.</p>",
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": ""
    },
    {
      "title": "Kubernetes v1.32 Adds A New CPU Manager Static Policy Option For Strict CPU Reservation",
      "url": "https://kubernetes.io/blog/2024/12/16/cpumanager-strict-cpu-reservation/",
      "date": 1734307200,
      "author": "",
      "unread": true,
      "content": "\n<p>In Kubernetes v1.32, after years of community discussion, we are excited to introduce a\n<code>strict-cpu-reservation</code> option for the <a href=\"https://kubernetes.io/docs/tasks/administer-cluster/cpu-management-policies/#static-policy-options\">CPU Manager static policy</a>.\nThis feature is currently in alpha, with the associated policy hidden by default. You can only use the\npolicy if you explicitly enable the alpha behavior in your cluster.</p>\n<h2 id=\"understanding-the-feature\">Understanding the feature</h2>\n<p>The CPU Manager static policy is used to reduce latency or improve performance. The <code>reservedSystemCPUs</code> defines an explicit CPU set for OS system daemons and kubernetes system daemons. This option is designed for Telco/NFV type use cases where uncontrolled interrupts/timers may impact the workload performance. you can use this option to define the explicit cpuset for the system/kubernetes daemons as well as the interrupts/timers, so the rest CPUs on the system can be used exclusively for workloads, with less impact from uncontrolled interrupts/timers. More details of this parameter can be found on the <a href=\"https://kubernetes.io/docs/tasks/administer-cluster/reserve-compute-resources/#explicitly-reserved-cpu-list\">Explicitly Reserved CPU List</a> page.</p>\n<p>If you want to protect your system daemons and interrupt processing, the obvious way is to use the <code>reservedSystemCPUs</code> option.</p>\n<p>However, until the Kubernetes v1.32 release, this isolation was only implemented for guaranteed\npods that made requests for a whole number of CPUs. At pod admission time, the kubelet only\ncompares the CPU <em>requests</em> against the allocatable CPUs. In Kubernetes, limits can be higher than\nthe requests; the previous implementation allowed burstable and best-effort pods to use up\nthe capacity of <code>reservedSystemCPUs</code>, which could then starve host OS services of CPU - and we\nknow that people saw this in real life deployments.\nThe existing behavior also made benchmarking (for both infrastructure and workloads) results inaccurate.</p>\n<p>When this new <code>strict-cpu-reservation</code> policy option is enabled, the CPU Manager static policy will not allow any workload to use the reserved system CPU cores.</p>\n<h2 id=\"enabling-the-feature\">Enabling the feature</h2>\n<p>To enable this feature, you need to turn on both the <code>CPUManagerPolicyAlphaOptions</code> feature gate and the <code>strict-cpu-reservation</code> policy option. And you need to remove the <code>/var/lib/kubelet/cpu_manager_state</code> file if it exists and restart kubelet.</p>\n<p>With the following kubelet configuration:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex;\"><span><span style=\"color:#008000;font-weight:bold\">kind</span>:<span style=\"color:#bbb\"> </span>KubeletConfiguration<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">apiVersion</span>:<span style=\"color:#bbb\"> </span>kubelet.config.k8s.io/v1beta1<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">featureGates</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span>...<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">CPUManagerPolicyOptions</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#a2f;font-weight:bold\">true</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">CPUManagerPolicyAlphaOptions</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#a2f;font-weight:bold\">true</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">cpuManagerPolicy</span>:<span style=\"color:#bbb\"> </span>static<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">cpuManagerPolicyOptions</span>:<span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"> </span><span style=\"color:#008000;font-weight:bold\">strict-cpu-reservation</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#b44\">&#34;true&#34;</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#008000;font-weight:bold\">reservedSystemCPUs</span>:<span style=\"color:#bbb\"> </span><span style=\"color:#b44\">&#34;0,32,1,33,16,48&#34;</span><span style=\"color:#bbb\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#bbb\"></span><span style=\"color:#00f;font-weight:bold\">...</span><span style=\"color:#bbb\">\n</span></span></span></code></pre></div><p>When <code>strict-cpu-reservation</code> is not set or set to false:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-console\" data-lang=\"console\"><span style=\"display:flex;\"><span><span style=\"color:#000080;font-weight:bold\">#</span> cat /var/lib/kubelet/cpu_manager_state\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">{&#34;policyName&#34;:&#34;static&#34;,&#34;defaultCpuSet&#34;:&#34;0-63&#34;,&#34;checksum&#34;:1058907510}\n</span></span></span></code></pre></div><p>When <code>strict-cpu-reservation</code> is set to true:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-console\" data-lang=\"console\"><span style=\"display:flex;\"><span><span style=\"color:#000080;font-weight:bold\">#</span> cat /var/lib/kubelet/cpu_manager_state\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#888\">{&#34;policyName&#34;:&#34;static&#34;,&#34;defaultCpuSet&#34;:&#34;2-15,17-31,34-47,49-63&#34;,&#34;checksum&#34;:4141502832}\n</span></span></span></code></pre></div><h2 id=\"monitoring-the-feature\">Monitoring the feature</h2>\n<p>You can monitor the feature impact by checking the following CPU Manager counters:</p>\n<ul>\n<li><code>cpu_manager_shared_pool_size_millicores</code>: report shared pool size, in millicores (e.g. 13500m)</li>\n<li><code>cpu_manager_exclusive_cpu_allocation_count</code>: report exclusively allocated cores, counting full cores (e.g. 16)</li>\n</ul>\n<p>Your best-effort workloads may starve if the <code>cpu_manager_shared_pool_size_millicores</code> count is zero for prolonged time.</p>\n<p>We believe any pod that is required for operational purpose like a log forwarder should not run as best-effort, but you can review and adjust the amount of CPU cores reserved as needed.</p>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>Strict CPU reservation is critical for Telco/NFV use cases. It is also a prerequisite for enabling the all-in-one type of deployments where workloads are placed on nodes serving combined control+worker+storage roles.</p>\n<p>We want you to start using the feature and looking forward to your feedback.</p>\n<h2 id=\"further-reading\">Further reading</h2>\n<p>Please check out the <a href=\"https://kubernetes.io/docs/tasks/administer-cluster/cpu-management-policies/\">Control CPU Management Policies on the Node</a>\ntask page to learn more about the CPU Manager, and how it fits in relation to the other node-level resource managers.</p>\n<h2 id=\"getting-involved\">Getting involved</h2>\n<p>This feature is driven by the <a href=\"https://github.com/Kubernetes/community/blob/master/sig-node/README.md\">SIG Node</a>. If you are interested in helping develop this feature, sharing feedback, or participating in any other ongoing SIG Node projects, please attend the SIG Node meeting for more details.</p>",
      "flags": null,
      "enclosureUrl": "",
      "enclosureMime": ""
    }
  ]
}